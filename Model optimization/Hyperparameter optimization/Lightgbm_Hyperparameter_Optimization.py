# -*- coding: utf-8 -*-
"""LightGBM_Hyperparameter_Optimization.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mO4sqHGKGKgvSaA9T81A-EZUFIrGa7th
"""

#LightGBM Hyperparameter Optimization
import lightgbm as lgb

# Replace spaces in feature names with underscores (only if necessary)
# This avoids issues caused by column names with spaces in LightGBM
X_train.columns = X_train.columns.str.replace(' ', '_')

# Define the LightGBM classifier with optimized parameters
lgb_model = lgb.LGBMClassifier(random_state=42,
                               force_col_wise=True,            # Optimize column-wise threading
                               boosting_type='gbdt',           # Use the default boosting type
                               scale_pos_weight=15804 / 19609,  # Handle class imbalance
                               n_jobs=-1)                      # Utilize all CPU cores

# Define a more focused parameter grid for hyperparameter tuning
parameters = {
    'max_depth': [5, 10],            # Refined range for tree depth
    'n_estimators': [50, 100],       # Focused on optimal range of estimators
    'learning_rate': [0.05, 0.1]     # Narrower learning rate range for efficiency
}

# Use GridSearchCV to perform hyperparameter tuning
cv = GridSearchCV(lgb_model, parameters, cv=3, n_jobs=-1, verbose=1)
cv.fit(X_train, y_train)

# Check the best parameters and the best score
print(f"Best Parameters: {cv.best_params_}")
print(f"Best Score: {cv.best_score_}")